{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "data = pd.read_csv(r'/home/manuwas/titanic/train.csv')\n",
    "data.head()\n",
    "\n",
    "#Fill age with median\n",
    "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
    "\n",
    "survived_sex = data[data['Survived']==1]['Sex'].value_counts()\n",
    "dead_sex = data[data['Survived']==0]['Sex'].value_counts()\n",
    "df = pd.DataFrame([survived_sex,dead_sex])\n",
    "df.index = ['Survived','Dead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature engineering .. \n",
    "\n",
    "def status(feature):\n",
    "\n",
    "    print ('Processing',feature,': ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combined_data():\n",
    "    # reading train data\n",
    "    train = pd.read_csv(r'/home/manuwas/titanic/train.csv')\n",
    "    \n",
    "    # reading test data\n",
    "    test = pd.read_csv(r'/home/manuwas/titanic/test.csv')\n",
    "\n",
    "    # extracting and then removing the targets from the training data \n",
    "    targets = train.Survived\n",
    "    train.drop('Survived',1,inplace=True)\n",
    "    \n",
    "\n",
    "    # merging train data and test data for future feature engineering\n",
    "    combined = train.append(test)\n",
    "    combined.reset_index(inplace=True)\n",
    "    combined.drop('index',inplace=True,axis=1)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined = get_combined_data()\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_titles():\n",
    "\n",
    "    global combined\n",
    "    \n",
    "    # we extract the title from each name\n",
    "    combined['Title'] = combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # a map of more aggregated titles\n",
    "    Title_Dictionary = {\n",
    "                        \"Capt\":       \"Officer\",\n",
    "                        \"Col\":        \"Officer\",\n",
    "                        \"Major\":      \"Officer\",\n",
    "                        \"Jonkheer\":   \"Royalty\",\n",
    "                        \"Don\":        \"Royalty\",\n",
    "                        \"Sir\" :       \"Royalty\",\n",
    "                        \"Dr\":         \"Officer\",\n",
    "                        \"Rev\":        \"Officer\",\n",
    "                        \"the Countess\":\"Royalty\",\n",
    "                        \"Dona\":       \"Royalty\",\n",
    "                        \"Mme\":        \"Mrs\",\n",
    "                        \"Mlle\":       \"Miss\",\n",
    "                        \"Ms\":         \"Mrs\",\n",
    "                        \"Mr\" :        \"Mr\",\n",
    "                        \"Mrs\" :       \"Mrs\",\n",
    "                        \"Miss\" :      \"Miss\",\n",
    "                        \"Master\" :    \"Master\",\n",
    "                        \"Lady\" :      \"Royalty\"\n",
    "\n",
    "                        }\n",
    "    \n",
    "    # we map each title\n",
    "    combined['Title'] = combined.Title.map(Title_Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_titles()\n",
    "combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = combined.groupby(['Sex','Pclass','Title'])\n",
    "grouped.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function drops the Name column since we won't be using it anymore because we created a Title column.\n",
    "#Then we encode the title values using a dummy encoding.\n",
    "\n",
    "def process_names():\n",
    "    \n",
    "    global combined\n",
    "    # we clean the Name variable\n",
    "    combined.drop('Name',axis=1,inplace=True)\n",
    "    \n",
    "    # encoding in dummy variable\n",
    "    titles_dummies = pd.get_dummies(combined['Title'],prefix='Title')\n",
    "    combined = pd.concat([combined,titles_dummies],axis=1)\n",
    "    \n",
    "    # removing the title variable\n",
    "    combined.drop('Title',axis=1,inplace=True)\n",
    "    \n",
    "    status('names')\n",
    "process_names()\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Process fare\n",
    "\n",
    "def process_fares():\n",
    "    \n",
    "    global combined\n",
    "    # there's one missing fare value - replacing it with the mean.\n",
    "    combined.Fare.fillna(combined.Fare.mean(),inplace=True)\n",
    "    \n",
    "    status('fare')\n",
    "    \n",
    "process_fares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This functions replaces the two missing values of Embarked with the most frequent Embarked value.\n",
    "def process_embarked():\n",
    "    \n",
    "    global combined\n",
    "    # two missing embarked values - filling them with the most frequent one (S)\n",
    "    combined.Embarked.fillna('S',inplace=True)\n",
    "    \n",
    "    # dummy encoding \n",
    "    embarked_dummies = pd.get_dummies(combined['Embarked'],prefix='Embarked')\n",
    "    combined = pd.concat([combined,embarked_dummies],axis=1)\n",
    "    combined.drop('Embarked',axis=1,inplace=True)\n",
    "    \n",
    "    status('embarked')\n",
    "\n",
    "process_embarked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function replaces NaN values with U (for Unknow). It then maps each Cabin value to the first letter.\n",
    "#Then it encodes the cabin values using dummy encoding again.\n",
    "\n",
    "def process_cabin():\n",
    "    \n",
    "    global combined\n",
    "    \n",
    "    # replacing missing cabins with U (for Uknown)\n",
    "    combined.Cabin.fillna('U',inplace=True)\n",
    "    \n",
    "    # mapping each Cabin value with the cabin letter\n",
    "    combined['Cabin'] = combined['Cabin'].map(lambda c : c[0])\n",
    "    \n",
    "    # dummy encoding ...\n",
    "    cabin_dummies = pd.get_dummies(combined['Cabin'],prefix='Cabin')\n",
    "    \n",
    "    combined = pd.concat([combined,cabin_dummies],axis=1)\n",
    "    \n",
    "    combined.drop('Cabin',axis=1,inplace=True)\n",
    "    \n",
    "    status('cabin')\n",
    "    \n",
    "process_cabin()\n",
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_sex():\n",
    "    \n",
    "    global combined\n",
    "    # mapping string values to numerical one \n",
    "    combined['Sex'] = combined['Sex'].map({'male':1,'female':0})\n",
    "    \n",
    "    status('sex')\n",
    "process_sex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_pclass():\n",
    "    \n",
    "    global combined\n",
    "    # encoding into 3 categories:\n",
    "    pclass_dummies = pd.get_dummies(combined['Pclass'],prefix=\"Pclass\")\n",
    "    \n",
    "    # adding dummy variables\n",
    "    combined = pd.concat([combined,pclass_dummies],axis=1)\n",
    "    \n",
    "    # removing \"Pclass\"\n",
    "    \n",
    "    combined.drop('Pclass',axis=1,inplace=True)\n",
    "    \n",
    "    status('pclass')\n",
    "    \n",
    "process_pclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This functions preprocess the tikets first by extracting the ticket prefix. When it fails in extracting a prefix it returns XXX.\n",
    "#Then it encodes prefixes using dummy encoding.\n",
    "\n",
    "def process_ticket():\n",
    "    \n",
    "    global combined\n",
    "    \n",
    "    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "    def cleanTicket(ticket):\n",
    "        ticket = ticket.replace('.','')\n",
    "        ticket = ticket.replace('/','')\n",
    "        ticket = ticket.split()\n",
    "        ticket = map(lambda t : t.strip() , ticket)\n",
    "        ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "        if len(ticket) > 0:\n",
    "            return ticket[0]\n",
    "        else: \n",
    "            return 'XXX'\n",
    "    \n",
    "\n",
    "    # Extracting dummy variables from tickets:\n",
    "\n",
    "    combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n",
    "    tickets_dummies = pd.get_dummies(combined['Ticket'],prefix='Ticket')\n",
    "    combined = pd.concat([combined, tickets_dummies],axis=1)\n",
    "    combined.drop('Ticket',inplace=True,axis=1)\n",
    "\n",
    "    status('ticket')\n",
    "    \n",
    "process_ticket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Processing family\n",
    "#This part includes creating new variables based on the size of the family (the size is by the way, another variable we create).\n",
    "#This creation of new variables is done under a realistic assumption: Large families are grouped together, hence they are more likely to get rescued than people traveling alone.\n",
    "\n",
    "def process_family():\n",
    "    \n",
    "    global combined\n",
    "    # introducing a new feature : the size of families (including the passenger)\n",
    "    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n",
    "    \n",
    "    # introducing other features based on the family size\n",
    "    combined['Singleton'] = combined['FamilySize'].map(lambda s : 1 if s == 1 else 0)\n",
    "    combined['SmallFamily'] = combined['FamilySize'].map(lambda s : 1 if 2<=s<=4 else 0)\n",
    "    combined['LargeFamily'] = combined['FamilySize'].map(lambda s : 1 if 5<=s else 0)\n",
    "    \n",
    "    status('family')\n",
    "\n",
    "# This function introduces 4 new features:\n",
    "# FamilySize : the total number of relatives including the passenger (him/her)self.\n",
    "# Sigleton : a boolean variable that describes families of size = 1\n",
    "# SmallFamily : a boolean variable that describes families of 2 <= size <= 4\n",
    "# LargeFamily : a boolean variable that describes families of 5 < size\n",
    "\n",
    "process_family()\n",
    "combined.shape\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All None Ages\n",
    "nan_age = combined[combined['Age'].isnull()] \n",
    "\n",
    "#All not null ages\n",
    "combined_agep = combined[combined['Age'].notnull()]\n",
    "combined_agep.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "train = combined_agep\n",
    "test = nan_age\n",
    "\n",
    "targets = copy.deepcopy(train.Age)\n",
    "train.drop('Age', axis=1, inplace = True)\n",
    "test.drop('Age', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# gnb = GaussianNB()\n",
    "\n",
    "# gnbfit = gnb.fit(train.astype(int), targets.astype(int))\n",
    "# predicted = gnbfit.predict(test.astype(int))\n",
    "# print type(predicted)\n",
    "# print len(predicted)\n",
    "# print predicted\n",
    "# new_df = pd.Series.to_frame(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True)\n",
    "# for i in range(2000000):\n",
    "#     clf.fit(train.astype(int), targets.astype(int))\n",
    "                    \n",
    "# predicted = clf.predict(test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_predicted = pd.DataFrame(np.array(predicted[:]), index = test.index,  columns = [\"Predicted_Age\"])\n",
    "# df_predicted.describe()\n",
    "#df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined.Age.fillna(combined['Age'].median(), inplace = True)\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scale all features\n",
    "\n",
    "def scale_all_features():\n",
    "    \n",
    "    global combined\n",
    "    \n",
    "    features = list(combined.columns)\n",
    "    features.remove('PassengerId')\n",
    "    combined[features] = combined[features].apply(lambda x: x/x.max(), axis=0)\n",
    "    \n",
    "    print ('Features scaled successfully !')\n",
    "    \n",
    "scale_all_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To evaluate our model we'll be using a 5-fold cross validation with the Accuracy metric.\n",
    "#To do that, we'll define a small scoring function.\n",
    "\n",
    "def compute_score(clf, X, y,scoring='accuracy'):\n",
    "    xval = cross_val_score(clf, X, y, cv = 5,scoring=scoring)\n",
    "    return np.mean(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recovering the train set and the test set from the combined dataset is an easy task.\n",
    "# train0 = pd.read_csv('/home/rahulmanuwas/Dropbox/Kaggle/train.csv')\n",
    "\n",
    "def recover_train_test_target():\n",
    "    global combined\n",
    "    \n",
    "    train0 = pd.read_csv('/home/manuwas/titanic/train.csv')\n",
    "    \n",
    "    targets = train0.Survived\n",
    "    train = combined.ix[0:890]\n",
    "    test = combined.ix[891:]\n",
    "    \n",
    "    return train,test,targets\n",
    "\n",
    "train,test,targets = recover_train_test_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "#In fact, feature selection comes with many benefits:\n",
    "#It decreases redundancy among the data\n",
    "#It speeds up the training process\n",
    "#It reduces overfitting\n",
    "#Tree-based estimators can be used to compute feature importances, which in turn can be used to discard irrelevant features.\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "clf = ExtraTreesClassifier(n_estimators=200)\n",
    "clf = clf.fit(train, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['feature'] = train.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "\n",
    "features.sort_values(by = ['importance'], ascending=False)\n",
    "\n",
    "# As you may notice, there is a great importance linked to Title_Mr, Age, Fare, and Sex.\n",
    "# There is also an important correlation with the Passenger_Id.\n",
    "# Let's now transform our train set and test set in a more compact datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_new = model.transform(train)\n",
    "train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_new = model.transform(test)\n",
    "test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters tuning\n",
    "# Random Forest \n",
    "forest = RandomForestClassifier(max_features='sqrt')\n",
    "\n",
    "parameter_grid = {\n",
    "                 'max_depth' : [4,5,6,7,8],\n",
    "                 'n_estimators': [200,210,240,250],\n",
    "                 'criterion': ['gini','entropy']\n",
    "                 }\n",
    "\n",
    "cross_validation = StratifiedKFold(targets, n_folds=5)\n",
    "\n",
    "grid_search = GridSearchCV(forest,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(train_new, targets)\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that the model is built by scanning several combinations of the hyperparameters, we can generate an output file to submit on Kaggle.\n",
    "\n",
    "output = grid_search.predict(test_new).astype(int)\n",
    "df_output = pd.DataFrame()\n",
    "df_output['PassengerId'] = test['PassengerId']\n",
    "df_output['Survived'] = output\n",
    "df_output[['PassengerId','Survived']].to_csv('/home/manuwas/result_a_RF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=10, warm_start=True)\n",
    "# for i in range(20000):\n",
    "#     clf.fit(train_new, targets)\n",
    "                    \n",
    "# output = clf.predict(test_new).astype(int)\n",
    "\n",
    "# df_output = pd.DataFrame()\n",
    "# df_output['PassengerId'] = test['PassengerId']\n",
    "# df_output['Survived'] = output\n",
    "# df_output[['PassengerId','Survived']].to_csv('/home/manuwas/result_a_ANN.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "# fit the model with data\n",
    "logreg.fit(train_new, targets)\n",
    "\n",
    "\n",
    "# predict the response values for the observations in X\n",
    "output = logreg.predict(test_new).astype(int)\n",
    "\n",
    "df_output = pd.DataFrame()\n",
    "df_output['PassengerId'] = test['PassengerId']\n",
    "df_output['Survived'] = output\n",
    "df_output[['PassengerId','Survived']].to_csv('/home/manuwas/result_a_LR.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
